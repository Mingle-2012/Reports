\section{Evaluation}
\label{sec:evaluation}

In this section, we evaluate the performance of our MapReduce framework using several test cases. We first describe the experimental setup and datasets in Section~\ref{subsec:experimental-setup} and Section~\ref{subsec:datasets}, respectively. We then present the results of the local test cases in Section~\ref{subsec:local-test-cases} and the cluster test cases in Section~\ref{subsec:cluster-test-cases}.

\subsection{Experimental Setup}
\label{subsec:experimental-setup}

In this part, we describe the experimental setup used to evaluate the performance of the implemented MapReduce framework. We run the framework on a single machine with multiple threads. It is equipped with an Intel Core i7-12700H CPU, 32 GB of RAM, and a 2 TB SSD. The software environment ran on Ubuntu 22.04 LTS in Windows Subsystem for Linux (WSL) 2.0., with Java being the primary programming language. We use JDK 21 for compiling and running the code, gRPC 1.68.0 for communication between the master and worker nodes, and JUnit 4.13.2 for testing. For the cluster test cases, we deploy the framework on the aforementioned local machine, deploying multiple worker nodes as separate docker containers. Each worker node runs on a separate thread, simulating a distributed environment. We use Docker 26.1.1 for containerization.

\begin{table}[!t]
    \centering
    \caption{Datasets used for evaluation.}
    \label{tab:datasets}
    \begin{tabular}{c c c}
        \hline
        \textbf{Dataset} & \textbf{Words} & \textbf{Size (KB)} \\
        \hline
        A Christmas Carol & 29,282 & 164 \\
        King Solomon's Mines & 82,197 & 444 \\
        Oliver Twist & 157,967 & 880 \\
        Random Words\textsuperscript{\dag} & 13,848,900 & 102,404 \\
        \hline
    \end{tabular}
    \vspace{0.5em} \\
    \footnotesize{\textsuperscript{\dag} Randomly generated datasets with different sizes.}
\end{table}

\subsection{Datasets}
\label{subsec:datasets}

We use four datasets for the evaluation of our MapReduce framework, as shown in Table~\ref{tab:datasets}. The first three datasets are classic novels, including \textit{A Christmas Carol}, \textit{King Solomon's Mines}, and \textit{Oliver Twist}. The fourth dataset is a random dataset with a size of 100 MB, which is generated by a Python script.

\subsection{Local Test Cases}
\label{subsec:local-test-cases}

In this part, we present the results of the local test cases conducted on the implemented MapReduce framework. 

\begin{table}[!t]
    \centering
    \caption{Performance of the WordCount application using our MapReduce implementation.}
    \label{tab:wordcount}
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{Dataset} & \textbf{Time (s)} \\
        \hline
        A Christmas Carol & 2.215 \\
        King Solomon's Mines & 5.67 \\
        Oliver Twist & 1.492 \\
        Random Words & 2.12 \\
        \hline
    \end{tabular}
\end{table}

\subsubsection{WordCount Application}

We first evaluate the performance of the WordCount application, a classic example in the MapReduce model. The WordCount application counts the frequency of each word in a given text file. We use a sample text file containing 1,000,000 words for this test case. The input data is split into 16 chunks, and each chunk is processed by a mapper. The intermediate key-value pairs are shuffled and reduced to generate the final word count. We measure the execution time of the WordCount application using different numbers of mappers and reducers. The results are shown in Table~\ref{tab:wordcount}.

\subsection{Cluster Test Cases}
\label{subsec:cluster-test-cases}