\section{Related Work}
\label{sec:relatedwork}

In distributed computing, Google File System (GFS) \cite{ghemawat2003google}, MapReduce \cite{dean2008mapreduce}, and Bigtable \cite{chang2008bigtable} are called "three pillars". GFS, released in 2003, provides a storage system for large-scale computing by breaking files into chunks and storing copies to ensure data reliability and availability, which is crucial for large-scale data processing. In 2004, MapReduce brought a new way of handling distributed computing. With its simple map and reduce operations, it addresses the challenges of parallelization and fault tolerance, letting developers focus on the main task. 

In 2006, Bigtable \cite{chang2008bigtable} added more capabilities with its structured data storage system. Using rows, columns, and timestamps to organize data, it allowed flexible and efficient querying, but its support for complex updates was limited.

In 2010, Google introduced Pregel \cite{malewicz2010pregel} and Dremel \cite{melnik2010dremel} to handle specific challenges. Pregel uses a method focused on vertices for large graph data, while Dremel uses one focused on columns to quickly process large datasets. But both systems are designed for specific tasks and lack of flexibility, compared to MapReduce.

In the same year, Google released Percolator to handle the problems of updating data incrementally. By using distributed transactions and notifications, Percolator makes it possible for faster updates and reduced delays. While it works better for small updates, larger batch jobs still relies on MapReduce. Around this time, the open-source Hadoop, launched in 2006, helped make MapReduce more widely used but still struggled with slow disk operations.

Spark \cite{zaharia2016apache} is almost known by everyone of the field by introducing in-memory computation and Resilient Distributed Datasets (RDDs), making tasks like repeated computations and interactive queries much faster. Spark reduces the need to rely on disk and made it easier for developers to handle complex tasks, quickly becoming a key tool in big data. From GFS’s storage system to MapReduce’s processing model, and tools like Pregel, Dremel, and Percolator, distributed computing has steadily improved. The limits of earlier systems have driven the creation of better and faster technologies.

